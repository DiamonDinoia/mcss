/**************************************************************************/
/*    Copyright (C) 2006 Romain Michalec                                  */
/*                                                                        */
/*    This library is free software; you can redistribute it and/or       */
/*    modify it under the terms of the GNU Lesser General Public          */
/*    License as published by the Free Software Foundation; either        */
/*    version 2.1 of the License, or (at your option) any later version.  */
/*                                                                        */
/*    This library is distributed in the hope that it will be useful,     */
/*    but WITHOUT ANY WARRANTY; without even the implied warranty of      */
/*    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU   */
/*    Lesser General Public License for more details.                     */
/*                                                                        */
/*    You should have received a copy of the GNU Lesser General Public    */
/*    License along with this library; if not, write to the Free Software   */
/*    Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,          */
/*    MA  02110-1301, USA                                                 */
/*                                                                        */
/**************************************************************************/

#include "ks-test.h"
#include <cstdlib>
#include <cmath>
#include <vector>
#include <iostream>

/* KOLMOGOROV-SMIRNOV TEST OF HOMOGENEITY
 *
 * This long explaination provides you with everything you must know to
 * understand totally the Kolmogorov-Smirnov test of homogeneity.
 *
 * When x = (x_1, ..., x_n) are the observed values (the "realisation")
 * of a random sample X = (X_1, ..., X_n), we define the "sample cumulative
 * distribution function", or "empirical cumulative distribution function"
 * of X, and denote it X_emp_cdf(x), as the proportion of observed values
 * in X that are less or equal to x. In other words, if k observed values
 * in the sample are less than or equal to x, then X_emp_cdf(x) = k/n.
 *
 * Thus X_emp_cdf is a step function, defined on R and with values
 * from 0 to 1, with jumps of magnitude 1/n at each observation.
 *
 * The (non-empirical) cumulative distribution function of X, X_cdf,
 * defined on R, with values from 0 to 1, is X_cdf(x) = Pr (X_k <= x),
 * with k in 1:n (it doesn't matter which one since they all have the
 * same distribution).
 *
 * The empirical c.d.f is of course an approximation of the c.d.f:
 *
 *     X_emp_cdf(x) --> X_cdf(x)   when n --> infty
 *
 * In fact, a much stronger result, known as the Glivenko-Cantelli theorem,
 * states that when sample size is large, X_emp_cdf converges uniformly
 * to X_cdf:
 *
 *     D_n = sup |X_emp_cdf(x) - X_cdf(x)| --> 0   when n --> infty
 *
 * the sup function is over all real numbers (i.e. sup_{x \in R}).
 *
 * We consider a problem in which random samples are taken from two
 * populations: sample1, or X_1, ..., X_n, from population 1; sample2,
 * or Y_1, ..., Y_m, from population 2.
 * We must determine, on the basis of the observed values in the
 * samples, whether they come from the same distribution or not
 * (hence the name "test of homogeneity").
 *
 * The hypotheses to be tested are as follow:
 *
 *     H0: sample1 and sample2 are taken from distributions having
 *         the same distribution function
 *     H1: sample1 and sample2 are taken from distributions having
 *         different distribution functions
 *
 * To determine which hypothesis is the most likely, we must shape
 * a test statistic. Since the sample c.d.f.'s appeared to be estimators
 * of the non-empirical c.d.f.'s, and since c.d.f.'s are caracteristic of
 * the distribution of their random variable (i.e., if two random variables
 * have same c.d.f., then they also have same d.f.), it seems a good idea
 * to base a test on the difference between the two sample c.d.f.'s, and take
 * large sample sizes to approximate the difference between the corresponding
 * distribution functions.
 *
 * Hence we consider the statistic D_nm defined as follows:
 *
 *     D_nm = sup | X_emp_cdf(x) - Y_emp_cdf(x) |
 *
 * When the null hypothesis H0 is true, X_cdf and Y_cdf are identical functions
 * and we can easily deduce from the Glivenko-Cantelli theorem, by bounding
 * |X_emp_cdf(x) - Y_emp_cdf(x)|, that:
 *
 *     D_nm --> 0 when n,m --> infty
 *
 * It seems therefore reasonable to use a test of the form:
 *
 *     Reject H0 if D_nm > some critical value c
 *
 * but in fact, we will use the following test:
 *
 *     Reject H0 if sqrt(n*m/(n+m))*D_nm > c
 *
 * because of the following result, established in 1933 by AndreÃ¯ Nikolaevitch
 * Kolmogorov and Vladimir Ivanovitch Smirnov:
 *
 *     For any t > 0,
 *     Pr (sqrt(n)*D_n <= t) -->
 *               1 - 2*Sum_{i=1:infty} (-1)^(i-1) exp(-2 i^2 t^2)
 *     when n --> infty
 *
 * a result that has been adapted to two-sample testing:
 *
 *     For any t > 0, if the null hypothesis H0 is true, then
 *     Pr (sqrt(n_approx)*D_nm <= t) -->
 *               1 - 2*Sum_{i=1:infty} (-1)^(i-1) exp(-2 i^2 t^2)
 *     when n,m --> infty and with n_approx = n*m/(n+m)
 *
 * We call "Kolmogorov's statistic" the statistic D_nm, and "limiting form
 * of Kolmogorov's statistic" the function
 *
 *     L(t) = 1 - 2*Sum_{i=1:infty} (-1)^(i-1) exp(-2 i^2 t^2)
 *
 * The result of Kolmogorov and Smirnov is of crucial importance, because
 * it enables the computation of Pr ( sqrt(nm/n+m)*D_nm <= t ) through the
 * (much easier) computation of the limiting form L(t), for large samples.
 *
 * As for Pearson chi-square test or Wilcoxon-Mann-Whitney ranks test,
 * we are interested in computing p-values for this test.
 *
 * For instance, let's suppose the observed values of sample1 and sample2
 * yield the value d for Kolmogorov's statistic D_nm, i.e. the value
 * sqrt(nm/n+m)*d for the statistic sqrt(nm/n+m)*D_nm. The very definition
 * of p as the significance level alpha corresponding to the critical value
 * c = sqrt(nm/n+m)*d reads:
 *
 *     p = Pr (H0 rejected | H0 true)
 *       = Pr (sqrt(nm/n+m)*D_nm > c = sqrt(nm/n+m)*d | H0 true)
 *       = Pr (D_nm > d | H0 true)
 *       = 1 - Pr (D_nm < d | H0 true)
 *
 * Moreover, we know that when H0 is true and sample sizes are large,
 *
 *     Pr (sqrt(nm/n+m)*D_nm < t) ~= L(t)
 *
 * i.e. with t = sqrt(nm/n+m)*d
 *
 *     Pr (D_nm < d) ~= L(sqrt(nm/n+m)*d)
 *
 * (we use ~= for "approximately equal to").
 *
 * Summary: once we have the value d of the test statistic D_nm (a value
 * that is really easy to obtain from the values in the samples),
 * we compute the p-value of the observed sample thanks to the formula
 * p = 1 - L(sqrt(nm/n+m)*d) (we stop the Sum in L when we think a
 * sufficient accuracy has been reached).
 *
 * In fact, in this implementation of Kolmogorov-Smirnov test of homogeneity,
 * we don't use the limiting form, but a small C procedure provided by
 * Marsaglia et al., K(int n,double d) (source code below), that computes
 * very quickly K(n,d) = Pr(D_n < d) with 13-15 digit accuracy -- much more
 * that what we need. The fact that it computes Pr(D_n < d) rather than
 * Pr(D_nm < d) is not a problem as we learned previously that both had the
 * same limiting forms, that samples sizes are large and that we do not
 * require a really high accuracy...
 *
 * Hence we have at last:
 *
 *     p = 1 - K(n_approx,d)    with  n_approx = nm/n+m
 *
 * Last thing to know: the differences between the one-sided and two-sided
 * versions of this test.
 *
 * "Reject H0 if sqrt(n*m/(n+m))*D_nm > c" is clearly one-sided; however,
 * because of the |.| in D_nm, the _meaning_ of the test is in fact two-sided.
 * Indeed, as D_nm = sup |X_emp_cdf(x) - Y_emp_cdf(x)|, rejecting H0 when
 * sqrt(nm/n+m)*D_nm > c means rejecting it when the "gap" between
 * X_emp_cdf(x) and Y_emp_cdf(x) is too large, in a direction or in the other,
 * i.e. X_emp_cdf(x) "above" or "under" Y_emp_cdf(x). Which translates
 * respectively into the X_k being globally smaller or larger than the Y_l.
 *
 * It is quite easy to make a two-sided test (with the meaning of a one-sided
 * test) from this one-sided test (with the meaning of a two-sided). This test
 * would be "Reject H0 if sqrt(nm/n+m)*D_nm != 0", or, better, the approximated
 * two-sided test: "Reject H_0 if |sqrt(nm/n+m)*D_nm| > epsilon", with epsilon
 * a very small number. However, this would not make much sense if D_nm was
 * still defined as sup |X_emp_cdf(x) - Y_emp_cdf(x)|; we should better
 * redefine it as D_nm = sup (X_emp_cdf(x) - Y_emp_cdf(x)).
 *
 * That would mean that, instead of rejecting H0 when the "gap" between
 * the c.d.f. X_emp_cdf(x) and Y_emp_cdf(x) is too large, in a direction or
 * in the other, we would reject it only if the "gap" is too large in the
 * sense of X_emp_cdf(x) being "above" Y_emp_cdf(x). Translation: when the
 * X_k are globally smaller than the Y_l. For instance, the X_k could be
 * the "old" traffic samples, and the Y_l the "new" ones, and we would
 * consider as anomalies only those that are more likely to be attacks
 * (increases in network traffic).
 *
 * As the p-value depends on the form of the test, we have to find another
 * formula for the p-value of this two-sided test, "Reject H_0 if
 * |sqrt(nm/n+m)*D_nm| > epsilon". Let's denote d the value of the
 * statistic D_nm yielded by the values in the sample, i.e. the value
 * of the statistic sqrt(nm/n+m)*D_nm is sqrt(nm/n+m)*d. Note that D_nm
 * is no longer Kolmogorov's statistic, but a modified Kolmogorov's statistic:
 * D_nm = sup (X_emp_cdf(x) - Y_emp_cdf(x)). As previously:
 *
 *     p = Pr (H0 rejected | H0 true)
 *       = Pr (|sqrt(nm/n+m)*D_nm| > sqrt(nm/n+m)*d | H0 true)
 *
 * Here we need a little drawing representing the distribution function of
 * the random variable sqrt(nm/n+m)*D_nm (let's assume it is something like
 * a regular curve from -infty to +infty, somewhat symetrical around 0 --
 * wishful thinking: there are no reasons it should be so):
 *
 *     p is the sum of the areas under the curve that are located further
 *     away from 0 than the value sqrt(nm/n+m)*d, i.e. from -infty to
 *     -sqrt(nm/n+m)*d and from sqrt(nm/n+m)*d to infty
 *
 * We suppose these areas are symetrical, each equals p/2 and we have:
 *
 *     1 - p/2 is the area under the curve from -infty to sqrt(nm/n+m)*d
 *
 * i.e.:
 *
 *     1 - p/2 = Pr (sqrt(nm/n+m)*D_nm < sqrt(nm/n+m)*d)
 *             = Pr (D_nm < d)
 *     p = 2 * (1 - Pr (D_nm < d))
 *
 * The difficulty resides in computing Pr (D_nm < d) now that D_nm is no
 * longer Kolmogorov's statistic... Neither the limiting form nor Marsaglia's
 * procedure are able to compute it, and although there exists a two-sample
 * version for Kolmogorov-Smirnov c.d.f. test, we are limited to the
 * one-sample version (unless we find one day a method to compute this
 * probability).
 */


/* The following three functions are copied from
 * G. Marsaglia, W. W. Tsang, J. Wang: "Evaluating  Kolmogorov's Distribution"
 *
 * The third one compute K(n,d) = Pr(D_n < d), where D_n is Kolmogorov's
 * goodness-of-fit measure for a sample c.d.f., n being the size of the
 * random sample: D_n = sup |X_emp_cdf(x) - X_cdf(x)| with the notations
 * of the previous text.
 *
 * The results correspond more or less to the tables found in
 * Hartung: "Statistik", 13rd Edition, pp. 521-523
 */
void mMultiply(double *A,double *B,double *C,int m)
{
    int i,j,k; double s;
    for(i=0;i<m;i++) for(j=0; j<m; j++)
        {s=0.; for(k=0;k<m;k++) s+=A[i*m+k]*B[k*m+j]; C[i*m+j]=s;}
}

void mPower(double *A,int eA,double *V,int *eV,int m,int n)
{
    double *B;int eB,i;
    if(n==1) {for(i=0;i<m*m;i++) V[i]=A[i];*eV=eA; return;}
    mPower(A,eA,V,eV,m,n/2);
    B=(double*)malloc((m*m)*sizeof(double));
    mMultiply(V,V,B,m); eB=2*(*eV);
    if(n%2==0){for(i=0;i<m*m;i++) V[i]=B[i]; *eV=eB;}
    else {mMultiply(A,B,V,m); *eV=eA+eB;}
    if(V[(m/2)*m+(m/2)]>1e140) {for(i=0;i<m*m;i++) V[i]=V[i]*1e-140;*eV+=140;}
    free(B);
}

double K(int n,double d)
{
    int k,m,i,j,g,eH,eQ;
    double h,s,*H,*Q;
    //OMIT NEXT LINE IF YOU REQUIRE >7 DIGIT ACCURACY IN THE RIGHT TAIL
    s=d*d*n; if(s>7.24||(s>3.76&&n>99)) return 1-2*exp(-(2.000071+.331/sqrt((double) n)+1.409/n)*s);
    k=(int)(n*d)+1; m=2*k-1; h=k-n*d;
    H=(double*)malloc((m*m)*sizeof(double));
    Q=(double*)malloc((m*m)*sizeof(double));
    for(i=0;i<m;i++) for(j=0;j<m;j++)
            if(i-j+1<0) H[i*m+j]=0; else H[i*m+j]=1;
    for(i=0;i<m;i++) {H[i*m]-=pow(h,i+1); H[(m-1)*m+i]-=pow(h,(m-i));}
    H[(m-1)*m]+=(2*h-1>0?pow(2*h-1,m):0);
    for(i=0;i<m;i++) for(j=0;j<m;j++)
            if(i-j+1>0) for(g=1;g<=i-j+1;g++) H[i*m+j]/=g;
    eH=0; mPower(H,eH,Q,&eQ,m,n);
    s=Q[(k-1)*m+k-1];
    for(i=1;i<=n;i++) {s=s*i/n; if(s<1e-140) {s*=1e140; eQ-=140;}}
    s*=pow(10.,eQ); free(H); free(Q); return s;
}

std::ostream & operator << (std::ostream & os, const std::list<int64_t> & L) {
    std::list<int64_t>::const_iterator it = L.begin();
    while (it != L.end()) {
        os << *it << ',';
        it++;
    }
    return os;
}

std::ostream & operator << (std::ostream & os, const std::list<double> & L) {
    std::list<double>::const_iterator it = L.begin();
    while (it != L.end()) {
        os << *it << ',';
        it++;
    }
    return os;
}

std::ostream & operator << (std::ostream & os, const std::vector<unsigned> & V) {
    std::vector<unsigned>::const_iterator it = V.begin();
    while (it != V.end()) {
        os << *it << ',';
        it++;
    }
    return os;
}

std::ostream & operator << (std::ostream & os, const std::vector<int64_t> & V) {
    std::vector<int64_t>::const_iterator it = V.begin();
    os << "( ";
    while (it != V.end()) {
        os << *it << " ";
        it++;
    }
    os << ")";
    return os;
}

std::ostream & operator << (std::ostream & os, const std::vector<double> & V) {
    std::vector<double>::const_iterator it = V.begin();
    os << "( ";
    while (it != V.end()) {
        os << *it << " ";
        it++;
    }
    os << ")";
    return os;
}

std::ostream & operator << (std::ostream & os, const std::list<std::vector<int64_t> > & L) {
    // We need a ckeck as L for sample_new is initially empty
    if (L.size() > 0) {
        std::list<std::vector<int64_t> >::const_iterator it = L.begin();
        os << *it; it++;
        while (it != L.end()) {
            os << ", " << *it;
            it++;
        }
    }
    return os;
}

std::ostream & operator << (std::ostream & os, const std::list<std::vector<double> > & L) {
    // We need a ckeck as L for sample_new is initially empty
    if (L.size() > 0) {
        std::list<std::vector<double> >::const_iterator it = L.begin();
        os << *it; it++;
        while (it != L.end()) {
            os << ", " << *it;
            it++;
        }
    }
    return os;
}

/* This function returns an approximated p-value of the Kolmogorov-Smirnov
 * c.d.f. test of homogeneity.
 *
 * The hypothesis to be tested are:
 *
 *     H0: the samples are drawn from populations having same distribution
 *     H1: they come from populations with different distributions
 *
 * The test statistic is (notations from the long explanation at the
 * beginning of this file):
 *
 *     D_nm = sup | X_emp_cdf(x) - Y_emp_cdf(x) |
 *
 * The p-value when the observed value for D_nm is d is computed through
 * (proof in the text at the beginning of this file):
 *
 *     p = 1 - Pr (D_nm < d)
 *       = 1 - K(n_approx,d)    with  n_approx = nm/n+m
 *
 * We reject H0 to significance level alpha if p-value < alpha,
 * i.e. considering the data we have, we reject H0 to any significance
 * level alpha > p-value.
 *
 * As explained at the end of the "opening text", Kolmogorov-Smirnov test
 * is a one-sided test (with a meaning of a two-sided), and the corresponding
 * two-sided test (with the meaning of a one-sided), although it exists and
 * makes sense, cannot be computed easily with our current means (Marsaglia's
 * procedure or the limiting form of D_nm).
 */
double ks_test (std::list<double> sample1, std::list<double> sample2,
                std::ostream & outfile) {

    unsigned int n1, n2, n_approx;
    // sample sizes
    float d;
    // the value of Kolmogorov's statistic
    // for the particular values of sample1 and sample2
    double D, Dmin, Dmax, s;
    // used in computing this value d

    std::list<double>::iterator it1, it2;

    // Determine sample sizes
    n1 = sample1.size();
    n2 = sample2.size();

    // Calculate a conservative n approximation
    n_approx = (unsigned) ceil(float(n1*n2)/(n1+n2));
    outfile << "n_approx=" << n_approx << std::endl;

    // Sort samples
    sample1.sort(); outfile << "sorted sample1: " << sample1 << std::endl;
    sample2.sort(); outfile << "sorted sample2: " << sample2 << std::endl;

    // We divide the range 0..1 into n1*n2 intervals of equal size 1/(n1*n2).
    //
    // Each item in sample1 makes the sample c.d.f of sample1
    // jump by a step of n2 intervals.
    // Each item in sample2 makes the sample c.d.f of sample2
    // jump by a step of n1 intervals.
    //
    // For each item we compute D, related to the distance between the two
    // sample c.d.f., s_cdf_1 - s_cdf_2, by:
    //
    //    D/(n1*n2) = s_cdf_1 - s_cdf_2
    //
    // We want to determine:
    //
    //    Dmin/(n1*n2) = min [s_cdf_1 - s_cdf_2] <= 0
    //    Dmax/(n1*n2) = max [s_cdf_1 - s_cdf_2] >= 0
    //
    // And then the value of Kolmorogov's statistic D_n1n2 is just:
    //
    //    D_n1n2 = sup |s_cdf_1 - s_cdf_2|
    //           = max [ |Dmin/(n1*n2)| ; |Dmax/(n1*n2)| ]

    D = 0; Dmin = 0; Dmax = 0;
    it1 = sample1.begin();
    it2 = sample2.begin();

    while ( (it1 != sample1.end()) && (it2 != sample2.end()) ) {

        if (*it1 == *it2) {

            outfile << *it1 << " tie!";
            // steps in both sample c.d.f., we need to perform all steps
            // in this point before comparing D to Dmin and Dmax

            s = *it1;
            // perform all steps in s_cdf_1 first
            do {
                D += n2;
                it1++;
            }
            while ( (*it1 == s) && (it1 != sample1.end()) );
            // perform all steps in s_cdf_2 now
            do {
                D -= n1;
                it2++;
            }
            while ( (*it2 == s) && (it2 != sample2.end()) );

            // now adapt Dmin, Dmax if necessary
            if (D > Dmax)
                Dmax = D;
            else if (D < Dmin)
                Dmin = D;

        }

        else if (*it1 < *it2) {

            outfile << *it1;
            // step in s_cdf_1, increase D by n2
            D += n2;
            it1++;

            if (D > Dmax)
                Dmax = D;

        }

        else {

            outfile << *it2;
            // step in F2, decrease D by n1
            D -= n1;
            it2++;

            if (D < Dmin)
                Dmin = D;

        }

        outfile << " D=" << D << " Dmin=" << Dmin << " Dmax=" << Dmax << std::endl;

    }

    // For two-sided test, take D = max (|Dmax|, |Dmin|) and compute
    // the value d of Kolmogorov's statistic (two-sided only)

    if (-Dmin > Dmax)
        D = -Dmin;
    else
        D = Dmax;

    // Hence the observed value of Kolmogorov's statistic:
    d = float(D)/(n1*n2);

    // Return p-value
    return 1 - K(n_approx,d);

}

